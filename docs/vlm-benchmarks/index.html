<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Medical LLM Research</title><style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:Arial,sans-serif;line-height:1.6;color:#333;background-color:#f4f4f9;overflow-x:hidden}a{color:#007bff;text-decoration:none}a:hover{text-decoration:underline}header{padding:1rem;text-align:center;background-color:#007bff;color:#fff;position:relative}header h1{margin-bottom:.5rem;font-size:1.5rem;word-wrap:break-word}nav{display:flex;justify-content:center;flex-wrap:wrap;gap:1rem;margin-top:.5rem}nav a{color:#fff;padding:.5rem 1rem;border-radius:4px;background-color:#ffffff1a;transition:background-color .3s}nav a:hover{background-color:#fff3;text-decoration:none}main{padding:1rem;background-color:#fff;min-height:calc(100vh - 120px);margin:0 auto;max-width:1200px}main h1,main h2,main h3{margin-top:1rem;word-wrap:break-word}main p{margin-bottom:1rem;word-wrap:break-word}main ul{margin-bottom:1rem;padding-left:1.5rem}main table{width:100%;border-collapse:collapse;margin-bottom:1rem;display:block;overflow-x:auto}main table th,main table td{border:1px solid #ddd;padding:.5rem;text-align:left;min-width:100px}main table th{background-color:#f4f4f9}.content-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:1rem;margin-bottom:2rem}.content-grid article{background-color:#fff;padding:1rem;border-radius:5px;box-shadow:0 2px 4px #0000001a;border:1px solid #eee}.content-grid h3{margin-bottom:.5rem}.content-grid p{margin-bottom:1rem}.button{display:inline-block;padding:.75rem 1.5rem;background-color:#007bff;color:#fff;text-decoration:none;border-radius:5px;transition:background-color .3s;border:none;font-size:1rem;cursor:pointer;min-width:200px;text-align:center}.button:hover{background-color:#0056b3;transform:translateY(-2px);box-shadow:0 4px 8px #0000001a}.quick-nav{display:flex;flex-direction:column;gap:.5rem;margin-top:2rem}.quick-nav a{display:block;padding:.75rem;background-color:#007bff;color:#fff;text-align:center;text-decoration:none;border-radius:5px;transition:background-color .3s;min-width:auto}.quick-nav a:hover{background-color:#0056b3}.menu-toggle{display:none;flex-direction:column;justify-content:space-between;width:30px;height:21px;background:transparent;border:none;cursor:pointer;padding:0;z-index:10}.hamburger span{display:block;height:3px;width:100%;background:#fff;border-radius:3px;transition:all .3s ease-in-out}@media(max-width:768px){header{padding:.75rem 1rem}header h1{font-size:1.25rem;margin-bottom:.25rem}nav{position:absolute;top:100%;left:0;width:100%;background-color:#007bff;flex-direction:column;align-items:center;padding:1rem 0;display:none;gap:.5rem}nav.active{display:flex}nav a{width:90%;text-align:center}.menu-toggle{display:flex;position:absolute;top:1.2rem;right:1rem}main{padding:1rem .5rem;min-height:calc(100vh - 100px)}.content-grid{grid-template-columns:1fr;gap:1rem}.quick-nav{flex-direction:column}.quick-nav a{width:100%}.button{min-width:150px;padding:.75rem 1rem}}@media(max-width:480px){header h1{font-size:1.1rem}main{padding:.75rem .25rem}.button{min-width:140px;padding:.6rem .8rem;font-size:.9rem}main table{font-size:.85rem}main table th,main table td{min-width:80px;padding:.3rem}}
</style></head> <body> <header> <h1>Medical LLM & VLM Benchmarks</h1> <button class="menu-toggle" aria-label="Toggle navigation menu"> <div class="hamburger"> <span></span> <span></span> <span></span> </div> </button> <nav> <a href="/research-medical-llm/">Home</a> <a href="/research-medical-llm/docs/llm-benchmarks/">LLM Benchmarks</a> <a href="/research-medical-llm/docs/vlm-benchmarks/">VLM Benchmarks</a> <a href="/research-medical-llm/docs/cost-analysis/">Cost Analysis</a> </nav> </header> <script type="module">document.addEventListener("DOMContentLoaded",function(){const n=document.querySelector(".menu-toggle"),e=document.querySelector("nav");n&&e&&(n.addEventListener("click",function(){e.classList.toggle("active")}),document.querySelectorAll("nav a").forEach(t=>{t.addEventListener("click",()=>{e.classList.remove("active")})}))});</script> <main> <h1 id="vlm-benchmarks-for-document-understanding">VLM Benchmarks for Document Understanding</h1>
<h3 id="1-fine-grained-perception-mmdocbench"><strong>1. Fine-Grained Perception: MMDocBench</strong></h3>
<p><strong>MMDocBench</strong> is a comprehensive benchmark designed to assess the <strong>fine-grained visual perception</strong> and reasoning abilities of VLMs in an OCR-free manner.</p>
<ul>
<li><strong>Composition:</strong> It defines <strong>15 main tasks</strong> with <strong>4,338 QA pairs</strong> and over 11,000 supporting regions.</li>
<li><strong>Document Diversity:</strong> It covers a wide array of document types, including <strong>research papers, receipts, financial reports, Wikipedia tables, charts, and infographics</strong>.</li>
<li><strong>Significance:</strong> Unlike object-level assessments in natural images, this benchmark uses multi-granularity information to evaluate how well models can reason over intricate document structures.</li>
</ul>
<h3 id="2-advanced-multimodal-reasoning-mmmu-pro-and-mmt-bench"><strong>2. Advanced Multimodal Reasoning: MMMU-Pro and MMT-Bench</strong></h3>
<p>As standard benchmarks like MMMU reached their limits, more rigorous versions were introduced to measure “true understanding”.</p>
<ul>
<li><strong>MMMU-Pro:</strong> This is an advanced version of the original MMMU. It introduces a <strong>vision-only input setting</strong> and increases the number of answer options from four to ten. It also simulates real-world conditions by using screenshots with varying backgrounds and font styles.</li>
<li><strong>MTT-Bench:</strong> This large-scale benchmark includes over <strong>31,000 multi-choice questions</strong> across 162 subtasks. It specifically evaluates capabilities in <strong>OCR, visual recognition, and visual-language retrieval</strong> across image, text, and video modalities.</li>
</ul>
<h3 id="3-specialized-document-tasks-vidore-and-funsd"><strong>3. Specialized Document Tasks: ViDoRe and FUNSD</strong></h3>
<p>The sources highlight benchmarks that focus on specific document processing workflows:</p>
<ul>
<li><strong>ViDoRe (Visual Document Retrieval):</strong> This benchmark is the standard for <strong>multimodal retrieval</strong> tasks. It evaluates how well models can find the most relevant page in a stack of PDFs (such as financial reports or scientific documents) based on a query. It features documents in both <strong>English and French</strong>.</li>
<li><strong>FUNSD:</strong> A well-known benchmark for <strong>form understanding in noisy, scanned documents</strong>. While valuable for its focus on heterogeneous elements like checkboxes and signatures, it is limited in scale, containing only <strong>199 forms</strong>.</li>
</ul>
<h3 id="4-performance-trends-in-document-understanding"><strong>4. Performance Trends in Document Understanding</strong></h3>
<ul>
<li><strong>Frontier Performance:</strong> Models like <strong>GLM-4.5V</strong> have demonstrated state-of-the-art results on 41 multimodal benchmarks, particularly excelling in analyzing clinical charts and complex spatial relationships using technologies like 3D-RoPE.</li>
<li><strong>Reasoning Models:</strong> Newer “thinking” models, such as <strong>Skywork R1V2</strong>, have achieved high scores on technically demanding benchmarks, including 73.6 on MMMU and 62.6 on <strong>OlympiadBench</strong>.</li>
<li><strong>VLM vs. Traditional OCR:</strong> In form-centric tasks, VLMs consistently outperform traditional systems like Tesseract and PP-OCR because they can jointly reason over visual regions and textual content rather than processing isolated text blocks.</li>
</ul>
<h3 id="sumber">Sumber</h3>
<ul>
  <li><a href="https://huggingface.co/blog/vlms-2025" target="_blank" rel="noopener noreferrer">Hugging Face Blog: Vision Language Models in 2025</a></li>
  <li><a href="https://arxiv.org/abs/2410.21311" target="_blank" rel="noopener noreferrer">arXiv Paper: MMDocBench</a></li>
  <li><a href="https://openreview.net/forum?id=WK6hQoAtgx" target="_blank" rel="noopener noreferrer">OpenReview: MMDocBench Submission</a></li>
  <li><a href="https://bnaic2025.unamur.be/accepted-submissions/accepted_oral/037%20-%20Synthetic%20Document%20Generation%20for%20Form%20Understanding%20with%20Vision-Language%20Models.pdf" target="_blank" rel="noopener noreferrer">BNAIC 2025: Synthetic Document Generation for Form Understanding</a></li>
</ul> </main> </body></html>
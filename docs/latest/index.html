<!DOCTYPE html><meta charset="utf-8"><style>
  .image-container {
    max-width: 100%;
    overflow: hidden;
  }

  .image-container img {
    width: 100%;
    height: auto;
  }
</style>
<ul>
<li><a href="#llm">Benchmark LLM</a></li>
<li><a href="#vlm">Benchmark VLM</a></li>
<li><a href="#pricing">Pricing</a></li>
</ul>
<h2 id="llm">LLM</h2>
<p>The performance landscape of medical large language models (LLMs) is currently divided between high-performing <strong>proprietary (closed-source)</strong> systems and rapidly advancing <strong>open-source (open-access)</strong> models. While proprietary models generally lead in raw benchmark scores, open-source models are closing the gap, particularly in specialized text analysis and privacy-sensitive clinical applications.</p>
<h3 id="1-proprietary-models-the-performance-leaders"><strong>1. Proprietary Models: The Performance Leaders</strong></h3>
<p>Proprietary models, such as those from Google and OpenAI, typically hold the highest scores on standardized medical exams due to their massive scale and closed development pipelines.</p>
<ul>
<li><strong>GPT-5:</strong> Currently the top-performing model across medical benchmarks.
<ul>
<li><strong>MedQA (USMLE):</strong> 95.84%.</li>
<li><strong>USMLE Step 2:</strong> 97.50%.</li>
<li><strong>MMLU Medical Genetics:</strong> 100%.</li>
<li><strong>MedXpertQA (Multimodal Reasoning):</strong> 69.99% (surpassing human experts at 45.76%).</li>
</ul>
</li>
<li><strong>Med-PaLM 2:</strong> A leading expert-level medical model from Google.
<ul>
<li><strong>MedQA (USMLE):</strong> 86.5%.</li>
<li><strong>PubMedQA:</strong> 81.8%.</li>
<li><strong>Human Preference:</strong> Preferred by physicians over human physician-generated answers on 8 of 9 clinical axes.</li>
</ul>
</li>
<li><strong>GPT-4 / GPT-4o:</strong> High-performing generalist models used as frequent baselines.
<ul>
<li><strong>GPT-4 (MedQA):</strong> 86.1% (base version).</li>
<li><strong>GPT-4o (MedQA):</strong> 91.04%.</li>
<li><strong>GPT-4o (Multimodal Reasoning):</strong> 40.73%.</li>
</ul>
</li>
</ul>
<h3 id="2-open-source-models-transparency-and-specialization"><strong>2. Open-Source Models: Transparency and Specialization</strong></h3>
<p>Open-source (or open-access) models provide public weights or code, making them suitable for local deployment and institutional research.</p>
<ul>
<li><strong>MEDITRON-70B:</strong> An open-access model adapted from Llama-2 specifically for medical reasoning.
<ul>
<li><strong>MedQA (4-option):</strong> 70.2% (using Self-Consistency Chain-of-Thought).</li>
<li><strong>PubMedQA:</strong> 81.6% (comparable to Med-PaLM 2).</li>
<li><strong>MedMCQA:</strong> 66.0%.</li>
</ul>
</li>
<li><strong>Me-LLaMA (70B-chat):</strong> A family of open-source models trained on 129 billion medical tokens, including clinical notes.
<ul>
<li><strong>MedQA:</strong> 62.3% (Supervised).</li>
<li><strong>Clinical Diagnosis:</strong> Comparable to ChatGPT and GPT-4 in diagnosing complex clinical cases (NEJM cases).</li>
<li><strong>Comprehensive Tasks:</strong> Excels in Relation Extraction (RE), Named Entity Recognition (NER), and natural language inference.</li>
</ul>
</li>
<li><strong>Other Open Baselines:</strong>
<ul>
<li><strong>PMC-LLaMA:</strong> 49.2% on MedQA (4-option).</li>
<li><strong>Llama-2-70B (Base):</strong> 61.3% on MedQA (4-option).</li>
<li><strong>Mistral-7B / Zephyr-7B:</strong> 41.1% and 48.5% on MedQA, respectively.</li>
</ul>
</li>
</ul>
<h3 id="comparison-table-benchmarks-at-a-glance"><strong>Comparison Table: Benchmarks at a Glance</strong></h3>




































































<table><thead><tr><th align="left">Model Type</th><th align="left">Model Name</th><th align="left">MedQA (USMLE)</th><th align="left">PubMedQA</th><th align="left">MedMCQA</th></tr></thead><tbody><tr><td align="left"><strong>Proprietary</strong></td><td align="left"><strong>GPT-5</strong></td><td align="left"><strong>95.8%</strong></td><td align="left">-</td><td align="left">-</td></tr><tr><td align="left"><strong>Proprietary</strong></td><td align="left"><strong>Med-PaLM 2</strong></td><td align="left"><strong>86.5%</strong></td><td align="left"><strong>81.8%</strong></td><td align="left"><strong>72.3%</strong></td></tr><tr><td align="left"><strong>Proprietary</strong></td><td align="left"><strong>GPT-4</strong></td><td align="left">81.4%—86.1%</td><td align="left">75.2%</td><td align="left">72.4%</td></tr><tr><td align="left"><strong>Proprietary</strong></td><td align="left"><strong>GPT-3.5 / ChatGPT</strong></td><td align="left">60.2%</td><td align="left">63.9%</td><td align="left">57.6%</td></tr><tr><td align="left"><strong>Open-Source</strong></td><td align="left"><strong>MEDITRON-70B</strong></td><td align="left"><strong>70.2%</strong></td><td align="left"><strong>81.6%</strong></td><td align="left">66.0%</td></tr><tr><td align="left"><strong>Open-Source</strong></td><td align="left"><strong>Me-LLaMA-70B</strong></td><td align="left">62.3%</td><td align="left">81.4%</td><td align="left">64.3%</td></tr><tr><td align="left"><strong>Open-Source</strong></td><td align="left"><strong>Llama-2-70B</strong></td><td align="left">59.2%</td><td align="left">78.0%</td><td align="left">62.7%</td></tr><tr><td align="left"><strong>Open-Source</strong></td><td align="left"><strong>PMC-Llama-13B</strong></td><td align="left">45.6%</td><td align="left">77.8%</td><td align="left">54.8%</td></tr></tbody></table>
<h3 id="analysis-of-key-concepts"><strong>Analysis of Key Concepts</strong></h3>
<ol>
<li><strong>Instruction Tuning vs. Pretraining:</strong> Research shows that while <strong>continual pretraining</strong> helps a model acquire deep domain knowledge (like Me-LLaMA or MEDITRON), <strong>instruction tuning</strong> is more cost-effective for improving immediate task performance in zero-shot settings.</li>
<li><strong>Multimodal Leap:</strong> GPT-5 represents a qualitative shift by outperforming human experts in integrating medical images with text, a task where previous models like GPT-4o struggled.</li>
<li><strong>The “Safety Gap”:</strong> While open-source models like Me-LLaMA and MEDITRON offer privacy for institutional data, they are not yet recommended for real-world deployment without extensive safety alignment and clinical trials.</li>
</ol>
<h3 id="sumber">Sumber:</h3>
<ul>
<li>Capabilities of GPT-5 on Multimodal Medical Reasoning - <a href="https://arxiv.org/abs/2508.08224">https://arxiv.org/abs/2508.08224</a></li>
<li>Towards Expert-Level Medical Question Answering with Large Language Models - <a href="https://arxiv.org/abs/2305.09617">https://arxiv.org/abs/2305.09617</a></li>
<li>Meditron-70B Scaling Medical Pretraining for Large Language Models - <a href="https://arxiv.org/abs/2311.16079">https://arxiv.org/abs/2311.16079</a></li>
<li>Medical foundation large language models for comprehensive text analysis and beyond - <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11882967/">https://pmc.ncbi.nlm.nih.gov/articles/PMC11882967/</a></li>
</ul>
<h2 id="vlm">VLM</h2>
<p>The sources identify <strong>GPT-5</strong> as the new state-of-the-art (SOTA) leader in multimodal reasoning, particularly in complex domains, while <strong>GPT-4o</strong> and <strong>Qwen-VL-Max</strong> remain top performers for general document understanding and spatial grounding.</p>
<h3 id="best-proprietary-vlms"><strong>Best Proprietary VLMs</strong></h3>
<p>Proprietary models lead the field in holistic reasoning and high-stakes diagnostic accuracy.</p>
<ul>
<li><strong>GPT-5:</strong> Described as a “generalist multimodal reasoner,” it achieves <strong>super-human proficiency</strong> in complex multimodal evaluations. It provides a “dramatic leap” over GPT-4o, improving multimodal reasoning and understanding scores by <strong>+29.26% and +26.18%</strong> respectively on the MedXpertQA MM benchmark. It is significantly more capable of synthesizing visual and textual cues to arrive at accurate clinical decisions.</li>
<li><strong>GPT-4o:</strong> Remains a top-tier model for <strong>answer prediction accuracy</strong> in general document reasoning, scoring highest on the MMDocBench (66.40% Exact Match).</li>
<li><strong>Qwen-VL-Max:</strong> The leading proprietary model for <strong>visual grounding (region prediction)</strong>, achieving the highest IOU score (11.44%) on MMDocBench. It is particularly strong in region prediction across general, table-based, and chart-based documents.</li>
<li><strong>Gemini-2.5-Flash:</strong> Cited as a high-performing generalist that sets a “high performance bar” on text-rich benchmarks like InfographicVQA and ChartQA.</li>
</ul>
<h3 id="best-open-source-vlms"><strong>Best Open-Source VLMs</strong></h3>
<p>Open-source models have become highly competitive, often surpassing proprietary models in specialized spatial tasks or specific reasoning categories.</p>
<ul>
<li><strong>Llava-OV-Chat-72B:</strong> The highest-performing open-source model for <strong>answer prediction</strong>, scoring 58.93% on MMDocBench. It uniquely outperforms all other models—proprietary or open—in <strong>Logical Reasoning</strong> and <strong>Spatial Reasoning</strong>.</li>
<li><strong>Qwen2.5-VL:</strong> A state-of-the-art generalist model that rivals proprietary suites in text-rich VQA and holistic image understanding.</li>
<li><strong>TextMonkey:</strong> The premier open-source model for <strong>fine-grained visual perception</strong>, specifically region prediction (19.22% IOU). It is the best model overall for interpreting <strong>infographic-based documents</strong>.</li>
<li><strong>InternVL2-Llama3-76B:</strong> Recognized as a leading open-source suite that closes the performance gap with commercial multimodal models.</li>
<li><strong>DAM-QA:</strong> The best-performing <strong>region-aware model</strong> among its peers (e.g., Shikra, Ferret), demonstrating significant gains in document-centric tasks (DocVQA) through its sliding-window architecture.</li>
</ul>
<h3 id="best-domain-specific-vlms"><strong>Best Domain-Specific VLMs</strong></h3>
<ul>
<li><strong>Medical/Clinical:</strong> <strong>MMedIns-Llama 3</strong> is a specialized open-source model that significantly outperforms GPT-4 and Claude-3.5 across a wide array of clinical tasks, including diagnosis, treatment planning, and information extraction.</li>
</ul>
<hr>
<p>To visualize the hierarchy, you can think of <strong>GPT-5</strong> as a <strong>senior consultant</strong> who has reached a level of insight that exceeds even trained human experts in complex cases, while <strong>Qwen-VL-Max</strong> and <strong>TextMonkey</strong> are like <strong>high-precision microscopes</strong> that are best at physically locating the smallest details on a slide.</p>
<h3 id="sumber-1">Sumber:</h3>
<ul>
<li>MMDocBench: benchmarking large vision-language models for fine-grained visual document understanding - <a href="https://arxiv.org/html/2410.21311v1">https://arxiv.org/html/2410.21311v1</a></li>
<li>Towards Evaluating and Building Versatile Large Language Models for Medicine - <a href="https://arxiv.org/abs/2408.12547">https://arxiv.org/abs/2408.12547</a></li>
<li>Describe Anything Model for Visual Question Answering on Text-rich Images - <a href="https://arxiv.org/html/2507.12441v1">https://arxiv.org/html/2507.12441v1</a></li>
</ul>
<h2 id="pricing">Pricing</h2>
<h3 id="a-harga-api-indikatif-2025">a. Harga API (indikatif 2025)</h3>
<p>Ringkasan kasar untuk 1M token (cek lagi portal resmi saat implementasi):</p>

































































<table><thead><tr><th>Model / Layanan</th><th>Tipe</th><th>Est. harga input / output per 1M token</th><th>Catatan</th></tr></thead><tbody><tr><td>Grok 4.1 Fast</td><td>API</td><td>~$0.20 / ~$0.50</td><td>Sangat murah untuk kelasnya, 2M context, cocok beban tinggi. [12]</td></tr><tr><td>Grok 3</td><td>API</td><td>~$3 / $15</td><td>Kelas GPT‑4o/Claude Sonnet. [10][11]</td></tr><tr><td>GPT‑4o</td><td>API</td><td>~$2.5 / $10</td><td>General + medis kuat, multimodal. [8][37][38]</td></tr><tr><td>GPT‑4o mini</td><td>API</td><td>~$0.15 / $0.60</td><td>Sangat murah, cukup untuk QA medis menengah. [34][36][8]</td></tr><tr><td>Claude 3.5 Sonnet</td><td>API</td><td>~$3 / $15</td><td>Reasoning panjang &#x26; teks kompleks. [9][8]</td></tr><tr><td>Claude Haiku (3.x/4.5)</td><td>API</td><td>~$1 / $5</td><td>Model kecil cepat. [35]</td></tr><tr><td>Med42‑70B (hosted/API)</td><td>API</td><td>kisaran premium setara GPT‑4o</td><td>Tergantung provider. [2][3][4]</td></tr><tr><td>GPT‑5</td><td>API</td><td>~$1.25 / $10</td><td>Input lebih murah dari GPT‑4o, performa jauh lebih tinggi. [39][40][41][42]</td></tr><tr><td>gpt‑5‑mini</td><td>API</td><td>~$0.25 / $2</td><td>Versi kecil, bagus untuk workload ringan. [39][40]</td></tr></tbody></table>
<p>Kalau dinormalisasi dengan <strong>indeks harga</strong> (GPT‑4o = 1.0) dan <strong>indeks performa medis</strong> (MedQA/USMLE‑style):</p>
<ul>
<li>GPT‑4o: price index ~1.0, performa medis ~85.[15][8]</li>
<li>Claude 3.5 Sonnet: p ~1.1, perf ~87.[9][8]</li>
<li>Grok 3: p ~1.0, perf ~80.[10][11][12]</li>
<li>Grok 4.1 Fast: p ~0.1, perf ~82.[12]</li>
<li>GPT‑4o mini: p ~0.06, perf ~75.[34][36][8]</li>
<li>Med42‑70B API: p ~1.0, perf ~90.[2][3][4][5]</li>
<li><strong>GPT‑5</strong>: p ~0.5–0.6, perf ~95–96.[13][14][17][39][15]</li>
</ul>
<p>Secara price‑to‑performance murni, GPT‑5 adalah titik yang sangat dominan: lebih murah per input token dari GPT‑4o tapi jauh lebih akurat, termasuk di benchmark medis yang AIMultiple pakai (MedQA).[1][14][39][13][15]</p>
<h4 id="sumber-2">Sumber:</h4>
<p><a href="https://www.nature.com/articles/s41746-024-01390-4">1</a>
<a href="https://gist.github.com/Teebor-Choka/a4a5b099b85404538e32eb8a06c71565">2</a>
<a href="https://kairntech.com/blog/articles/llm-on-premise/">3</a>
<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11638254/">4</a>
<a href="https://basebox.ai/blog/local-ai-enterprise-scale-practical-insights-and-tooling-for-on-premise-llms">5</a>
<a href="https://aclanthology.org/2024.bionlp-1.14.pdf">6</a>
<a href="https://costgoat.com/pricing/grok-api">7</a>
<a href="https://arxiv.org/html/2507.12441v1">8</a>
<a href="https://www.vantage.sh/blog/gpt-4o-small-vs-gemini-1-5-flash-vs-claude-3-haiku-cost">9</a>
<a href="https://arxiv.org/abs/2410.21311">10</a>
<a href="https://mmdocbench.github.io">11</a>
<a href="https://huggingface.co/m42-health/Llama3-Med42-8B">12</a>
<a href="https://www.emergentmind.com/topics/med42-llama3-1-70b">13</a>
<a href="https://arxiv.org/html/2408.06142v1">14</a>
<a href="https://openreview.net/pdf?id=oulcuR8Aub">15</a>
<a href="https://openreview.net/pdf?id=ZcD35zKujO">16</a>
<a href="https://ai.meta.com/blog/llama-2-3-meditron-yale-medicine-epfl-open-source-llm/">17</a>
<a href="https://intuitionlabs.ai/articles/ai-api-pricing-comparison-grok-gemini-openai-claude">18</a>
<a href="https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/vlm.html">19</a>
<a href="https://www.scribd.com/document/662752205/DOCVQA1">20</a>
<a href="https://openaccess.thecvf.com/content/ICCV2025W/VisionDocs/papers/Vu_Describe_Anything_Model_for_Visual_Question_Answering_on_Text-rich_Images_ICCVW_2025_paper.pdf">21</a>
<a href="https://aclanthology.org/2024.findings-acl.78.pdf">22</a>
<a href="https://pieces.app/blog/how-to-use-gpt-4o-gemini-1-5-pro-and-claude-3-5-sonnet-free">23</a>
<a href="https://docsbot.ai/models/compare/gpt-4o-mini/claude-3-5-haiku">24</a>
<a href="https://skywork.ai/blog/claude-haiku-4-5-vs-gpt4o-mini-vs-gemini-flash-vs-mistral-small-vs-llama-comparison/">25</a>
<a href="https://blog.promptlayer.com/big-differences-claude-3-5-vs-gpt-4o/">26</a>
<a href="https://www.linkedin.com/pulse/xai-launches-grok-3-api-four-pricing-tiers-intensifying-%E6%9D%B0-%E9%82%93-q1qic">27</a>
<a href="https://apidog.com/blog/grok-4-pricing/">28</a>
<a href="https://www.reddit.com/r/LocalLLaMA/comments/1iglg8t/need_advice_on_hardware_for_running_a_70b_local/">29</a>
<a href="https://www.reddit.com/r/LocalLLaMA/comments/1eiwnqe/hardware_requirements_to_run_llama_3_70b_on_a/">30</a>
<a href="https://www.ijcrt.org/papers/IJCRT2512402.pdf">31</a>
<a href="https://nanonets.com/blog/vision-language-model-vlm-for-data-extraction/">32</a>
<a href="https://arxiv.org/html/2510.15727v2">33</a>
<a href="https://arxiv.org/html/2404.14779v1">34</a>
<a href="https://the-rogue-marketing.github.io/grok-api-latest-llms-pricing-october-2025/">35</a>
<a href="https://www.businesswaretech.com/blog/research-ai-models-invoice-processing-benchmark">36</a>
<a href="https://github.com/SCUT-DLVCLab/Document-AI-Recommendations/blob/main/Approaches/approaches_vie.md">37</a>
<a href="https://unstructured.io/blog/benchmarking-document-parsing-and-what-actually-matters">38</a>
<a href="https://www.johnsnowlabs.com/visual-document-understanding-benchmark-comparative-analysis-of-in-house-and-cloud-based-form-extraction-models/">39</a>
<a href="https://raw.githubusercontent.com/mlresearch/v260/main/assets/he25a/he25a.pdf">40</a></p>
<h3 id="b-scatter-plot-gambaran-visual-price-vs-performance">b. Scatter plot (gambaran visual price vs performance)</h3>
<div class="image-container">
<p><img  src="/research-medical-llm-html/_astro/plot.MtDR0eQb_ufzRL.webp" alt="plot" loading="lazy" decoding="async" fetchpriority="auto" width="2400" height="1600"></p>
</div>
<h2 id="ringkasan-plot-x--indeks-harga-lebih-murah-ke-kiri-y--skor-medqa-lebih-tinggi-ke-atas">Ringkasan plot (X = indeks harga lebih murah ke kiri, Y = skor MedQA lebih tinggi ke atas)</h2>
<ul>
<li>
<p><strong>Dominan points</strong> (sweet spot kanan atas tapi tidak terlalu mahal):</p>
<ul>
<li><strong>GPT‑5 API</strong> (~0.55 harga, 96 skor) → Paling unggul P2P, sedikit lebih mahal dari GPT‑4o tapi akurasi jauh lebih tinggi.[1][2][3][4]</li>
<li><strong>GPT‑5 VLM</strong> (~0.65 harga, 95 skor) → Untuk multimodal (dokumen + gambar); cost sedikit naik karena vision input.[2][5][6]</li>
<li><strong>Med42‑70B API/on‑prem</strong> (1.0/0.3 harga, 90 skor) → Open medical LLM kuat; on‑prem menang kalau volume tinggi.[7][8][9][10][11]</li>
</ul>
</li>
<li>
<p><strong>Cost leaders</strong> (kiri bawah, murah tapi cukup pintar):</p>
<ul>
<li><strong>GPT‑4o mini</strong> (0.06, 75)</li>
<li><strong>Grok 4.1 Fast</strong> (0.1, 82)</li>
<li><strong>Med42‑8B on‑prem</strong> (0.08, 78) → Bagus untuk triage lokal.[10][12][13][14]</li>
</ul>
</li>
<li>
<p><strong>Premium/high perf tapi mahal</strong> (kanan atas):</p>
<ul>
<li><strong>Med‑PaLM 2</strong> (1.2, 87) → Kuat clinical QA tapi harga enterprise.[15][16][17]</li>
<li><strong>Claude 3.5 Sonnet</strong> (1.1, 87)</li>
</ul>
</li>
<li>
<p><strong>Base model</strong> (tidak direkomendasikan tanpa tuning):</p>
<ul>
<li><strong>Llama 3 70B base on‑prem</strong> (0.25, 65) → Fondasi Med42/Meditron, tapi QA medis lemah tanpa fine‑tuning.[18][19][20]</li>
</ul>
</li>
</ul>
<h2 id="insight">Insight</h2>
<ul>
<li><strong>Untuk volume rendah–menengah</strong>: <strong>GPT‑5 API/VLM</strong> jadi pilihan utama (terbaik P2P absolut).[1][2][3][4]</li>
<li><strong>Untuk volume tinggi + data residency</strong>: <strong>Med42‑70B on‑prem</strong> atau <strong>Med42‑8B</strong> untuk triage → eskalasi ke GPT‑5 API hanya kasus sulit.[7][8][9][10][11]</li>
<li><strong>Hybrid ideal</strong>: Gunakan plot ini untuk “tiering” – murah untuk 80% workload sederhana, premium untuk 20% kompleks.</li>
</ul>
<h4 id="sumber-3">Sumber:</h4>
<p><a href="https://www.cursor-ide.com/blog/gpt-5-api">1</a>
<a href="https://arxiv.org/abs/2508.08224">2</a>
<a href="https://binaryverseai.com/gpt-5-medical-2025-studies-multimodal-mri/">3</a>
<a href="https://www.ainews.com/p/gpt-5-surpasses-doctors-in-medical-reasoning-benchmarks">4</a>
<a href="https://arxiv.org/html/2410.21311v1">5</a>
<a href="https://openreview.net/forum?id=WK6hQoAtgx">6</a>
<a href="https://huggingface.co/m42-health/Llama3-Med42-8B">7</a>
<a href="https://www.emergentmind.com/topics/med42-llama3-1-70b">8</a>
<a href="https://arxiv.org/html/2408.06142v1">9</a>
<a href="https://huggingface.co/m42-health/Llama3-Med42-70B">10</a>
<a href="https://skywork.ai/blog/models/llama3-med42-70b-free-chat-online-skywork-ai/">11</a>
<a href="https://costgoat.com/pricing/grok-api">12</a>
<a href="https://docsbot.ai/models/compare/gpt-4o-mini/claude-3-5-haiku">13</a>
<a href="https://www.vantage.sh/blog/gpt-4o-small-vs-gemini-1-5-flash-vs-claude-3-haiku-cost">14</a>
<a href="https://arxiv.org/pdf/2305.09617.pdf">15</a>
<a href="https://dr7.ai/blog/model/top-5-medical-ai-models-compared-medgemma-gpt-4-med-palm-2-more/">16</a>
<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11922739/">17</a>
<a href="https://openreview.net/pdf?id=ZcD35zKujO">18</a>
<a href="https://ai.meta.com/blog/llama-2-3-meditron-yale-medicine-epfl-open-source-llm/">19</a>
<a href="https://arxiv.org/pdf/2311.16079.pdf">20</a></p>